from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random
import time
import math
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
plt.switch_backend('agg')
import matplotlib.ticker as ticker
import numpy as np
from os import system
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
import json
import os 
import matplotlib.pyplot as plt


"""========================================================================================
The sample.py includes the following template functions:
1. Encoder, decoder
2. Training function
3. BLEU-4 score function
You have to modify them to complete the lab.
In addition, there are still other functions that you have to 
implement by yourself.
1. Your own dataloader (design in your own way, not necessary Pytorch Dataloader)
2. Output your results (BLEU-4 score, correction words)
3. Plot loss/score
4. Load/save weights
========================================================================================"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SOS_token = 0
EOS_token = 1
#----------Hyper Parameters----------#
MAX_LENGTH = 0
hidden_size = 256
vocab_size = 28
teacher_forcing_ratio = 0.8
LR = 0.01
epoch = 200 
dictionary1 = {'SOS':0,'EOS':1,'UNK':2,'a':3,'b':4 ,'c':5,'d':6,'e':7,'f':8,'g':9,'h':10,'i':11,'j':12,'k':13,
               'l':14,'m':15,'n':16,'o':17,'p':18,'q':19,'r':20,'s':21,'t':22,'u':23,'v':24,'w':25,'x':26,'y':27,'z':28}
dictionary2 = {0:'SOS',1:'EOS',2:'UNK',3:'a',4:'b' ,5:'c',6:'d',7:'e',8:'f',9:'g',10:'h',11:'i',12:'j',13:'k',
               14:'l',15:'m',16:'n',17:'o',18:'p',19:'q',20:'r',21:'s',22:'t',23:'u',24:'v',25:'w',26:'x',27:'y',28:'z'}
################################
#Example inputs of compute_bleu
################################
#The target word
reference = 'variable'
#The word generated by your model
output = 'varable'

def char2int(sequence,add_eos=True):
    ints = []
    for char in sequence:
        ints.append(dictionary1[char])
        if add_eos:
            ints.append(dictionary1['EOS'])
        MAX_LENGTH = len(ints)
    return ints

def int2char(ints):
    words = ""
    for i in ints:
        words+=dictionary2[i]
    return words

def building_model(path):
    int_list=[]
    str_list=[]
    with open(path,'r') as file:
        dict_list=json.load(file)
        for dict in dict_list:
            target=char2int(dict['target'])
            for input in dict['input']:
                int_list.append([char2int(input,add_eos=True),target])
                str_list.append([input,dict['target']])
    return int_list,str_list

#compute BLEU-4 score
def compute_bleu(output, reference):
    cc = SmoothingFunction()
    if len(reference) == 3:
        weights = (0.33,0.33,0.33)
    else:
        weights = (0.25,0.25,0.25,0.25)
    return sentence_bleu([reference], output,weights=weights,smoothing_function=cc.method1)

def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

#Encoder
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size)

    def forward(self, input, hidden_state, cell_state):
        embedded = self.embedding(input).view(1, 1, -1)  
        output,(hidden_state,cell_state) = self.lstm(embedded, (hidden_state,cell_state) )
        return output,hidden_state,cell_state

    def initHidden(self):
        (torch.zeros(1, 1, self.hidden_size, device=device), torch.zeros(1, 1, self.hidden_size, device=device))
   
#Decoder
class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(output_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)
    
    def forward(self, input, hidden_state, cell_state):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output, (hidden_state,cell_state) = self.rnn(output, (hidden_state,cell_state) )
        output = self.softmax(self.out(output[0]))
        return output,hidden_state,cell_state

    def initHidden(self):
        (torch.zeros(1, 1, self.hidden_size, device=device), torch.zeros(1, 1, self.hidden_size, device=device))
   
def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden()

    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    #----------sequence to sequence part for encoder----------#
    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)


    decoder_input = torch.tensor([[SOS_token]], device=device)

    decoder_hidden = encoder_hidden

    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False
	

    #----------sequence to sequence part for decoder----------#
    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # Teacher forcing

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS_token:
                break

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return loss.item() / target_length

def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    # your own dataloader
    for input_tensor,target_tensor in training_pairs:
        loss = train(decoder_type,input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion,max_length,teacher_forcing_ratio,device)
        loss_total+=loss

    criterion = nn.CrossEntropyLoss()

    for iter in range(1, n_iters + 1):
        training_pair = training_pairs[iter - 1]
        input_tensor = training_pair[0]
        target_tensor = training_pair[1]

        loss = train(input_tensor, target_tensor, encoder,
                     decoder, encoder_optimizer, decoder_optimizer, criterion)
        print_loss_total += loss
        plot_loss_total += loss

        if iter % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),
                                         iter, iter / n_iters * 100, print_loss_avg))

def evaluate(input_tensor,encoder,decoder,max_length,device):
    predicted,predicted_list=[]
    input_length=input_tensor.size(0)
    encoder_outputs=torch.zeros(max_length,encoder.hidden_size,device=device)
   
    encoder_hidden = encoder.initHidden();
    for input_tensor,target_tensor in testing_pairs:
        predicted_list.append(predicted)
    return predicted_list

training_list,_= building_model('train.json')
training_tensor_list = []

for training_pair in training_list:
    input_tensor = torch.tensor(training_pair[0], device=device).view(-1, 1)
    target_tensor = torch.tensor(training_pair[1], device=device).view(-1, 1)
    training_tensor_list.append((input_tensor, target_tensor))

testing_list,testing_input=building_model('test.json')
testing_tensor_list=[]

for testing_pair in testing_list:
    input_tensor=torch.tensor(testing_pair[0],device=device).view(-1,1)
    target_tensor=torch.tensor(testing_pair[1],device=device).view(-1,1)
    testing_tensor_list.append((input_tensor,target_tensor))

encoder = EncoderRNN(vocab_size, hidden_size).to(device)
decoder = DecoderRNN(hidden_size, vocab_size).to(device)

